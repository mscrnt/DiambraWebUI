{
    "n_steps": {
        "title": "Training Step Count",
        "description": "Defines the number of steps the agent takes in the environment before pausing to update its learning. Larger values allow the agent to explore more thoroughly before optimizing its strategy.",
        "example": "Example: Set to 2048 to let the agent explore extensively before adjusting its behavior.",
        "proTip": "For smoother training, use values between 1024 and 8192. Increase this if the agent seems to struggle with complex scenarios."
    },
    "batch_size": {
        "title": "Batch Size",
        "description": "Specifies the number of experiences grouped together for training. Larger batch sizes lead to more stable updates but require more memory.",
        "example": "Example: Use a batch size of 64 or 128 for efficient training that balances speed and resource usage.",
        "proTip": "Start with 64 or 128 and adjust based on performance and memory availability. Larger batches generally lead to better gradient estimates."
    },
    "n_epochs": {
        "title": "Epochs per Update",
        "description": "Determines how many times the agent revisits its collected experiences during a single update. More epochs lead to better learning but may increase training time.",
        "example": "Example: Set to 5 for a balanced approach to revisiting experiences.",
        "proTip": "Increase this value if learning seems incomplete. However, avoid excessively high numbers to prevent overfitting."
    },
    "gamma": {
        "title": "Discount Factor",
        "description": "Controls how much importance the agent places on future rewards versus immediate ones. Higher values prioritize long-term strategies.",
        "example": "Example: Set to 0.99 to encourage long-term planning, such as completing entire game levels.",
        "proTip": "Stick close to 1 for environments requiring foresight, and reduce slightly for tasks emphasizing immediate actions."
    },
    "gae_lambda": {
        "title": "GAE Lambda",
        "description": "Fine-tunes the balance between learning from recent experiences and considering longer-term rewards. Higher values emphasize smoother, longer-term strategies.",
        "example": "Example: Use 0.95 to balance short- and long-term decision-making effectively.",
        "proTip": "For most environments, a value around 0.95 works well. Decrease slightly if the agent needs quicker responses to recent events."
    },
    "clip_range_start": {
        "title": "Initial Clipping Range",
        "description": "Defines the allowable range of policy updates during the early stages of training. Helps prevent large, destabilizing changes.",
        "example": "Example: Start with 0.2 to allow moderate experimentation while maintaining stability.",
        "proTip": "If training diverges, try lowering this value. A range of 0.1 to 0.3 works well for most cases."
    },
    "clip_range_end": {
        "title": "Final Clipping Range",
        "description": "Restricts policy updates in the later stages of training, ensuring fine-tuning without drastic adjustments.",
        "example": "Example: Set to 0.05 for precise refinements as training concludes.",
        "proTip": "Always set this lower than the starting clip range. A final value between 0.01 and 0.1 is typically effective."
    },
    "vf_coef": {
        "title": "Value Function Coefficient",
        "description": "Adjusts the weight of the value function during training, which helps the agent understand the broader context of its actions.",
        "example": "Example: Use 0.5 to balance value estimation with immediate policy improvements.",
        "proTip": "If the agent struggles with long-term planning, increase this value. Reduce it if short-term reactions are more critical."
    },
    "ent_coef": {
        "title": "Exploration Factor",
        "description": "Encourages the agent to explore new strategies rather than repeating familiar ones. A small amount of exploration helps avoid getting stuck.",
        "example": "Example: Set to 0.01 for controlled exploration.",
        "proTip": "Increase slightly if the agent is stuck in repetitive behaviors. Lower it if exploration becomes excessive and detracts from optimization."
    },
    "max_grad_norm": {
        "title": "Gradient Norm Clipping",
        "description": "Limits the size of updates to the agent’s policy, preventing drastic changes that can destabilize training.",
        "example": "Example: Set to 0.5 for steady and controlled updates during training.",
        "proTip": "Lower this value if training becomes erratic. Values between 0.5 and 1.0 work well in most cases."
    },
    "normalize_advantage": {
        "title": "Advantage Normalization",
        "description": "Enabling this ensures the agent evaluates its actions relative to the average. This helps it focus on the most impactful decisions.",
        "example": "Example: Set this to True to help the agent learn faster by comparing its actions effectively.",
        "proTip": "Keep this enabled for most scenarios to ensure optimal training performance."
    },
    "device": {
        "title": "Compute Device",
        "description": "Specifies whether training runs on a CPU or GPU. A GPU significantly speeds up training when available.",
        "example": "Example: Use 'auto' to let the platform decide the fastest option.",
        "proTip": "Set to 'cuda' if you have a GPU for maximum performance, or 'cpu' for smaller-scale experiments."
    },
    "num_envs": {
        "title": "Parallel Environments",
        "description": "Defines the number of environments running simultaneously during training. More environments lead to faster data collection.",
        "example": "Example: Use 8 or 16 environments for faster training progress.",
        "proTip": "Start with 8 environments and increase based on your system’s memory and processing power."
    },
    "total_timesteps": {
        "title": "Total Training Timesteps",
        "description": "Sets the total number of steps the agent trains for across all environments. More steps lead to better mastery but take longer.",
        "example": "Example: Set to 1,000,000 (1e6) for a robust training session on medium-difficulty tasks.",
        "proTip": "Use larger values like 2,000,000 for challenging scenarios to give the agent ample time to learn."
    },
    "autosave_freq": {
        "title": "Autosave Frequency",
        "description": "Determines how often the training progress is saved, ensuring no data is lost during long sessions.",
        "example": "Example: Save every 100,000 steps to preserve progress regularly.",
        "proTip": "Save more frequently during experiments or with unstable setups to avoid losing progress."
    },
    "random_stages": {
        "title": "Randomized Stages",
        "description": "Enables the agent to train on a variety of levels instead of focusing on just one. Helps improve adaptability.",
        "example": "Example: Enable to allow the agent to train on diverse scenarios like boss fights and tricky platforming sections.",
        "proTip": "Keep enabled for more comprehensive training. Disable only if focusing on a specific scenario."
    },
    "stages": {
        "title": "Specific Training Stages",
        "description": "Lists the stages the agent trains on when randomization is enabled. Guides the agent’s focus to key levels.",
        "example": "Example: ['Level 1', 'Level 2'] to target specific challenges.",
        "proTip": "Select levels that highlight different aspects of the game for more balanced training."
    },
    "wrappers": {
        "title": "Environment Wrappers",
        "description": "Custom tools added to the environment to modify observations, rewards, or actions. These enhance training and require programming expertise.",
        "example": "Example: Use a wrapper to normalize observations or add custom reward signals.",
        "proTip": "Advanced users can design wrappers to tailor training to specific needs. Refer to the documentation for setup details."
    },
    "callbacks": {
        "title": "Training Callbacks",
        "description": "Callbacks allow you to integrate custom functions into the training process, such as monitoring performance, saving progress, or stopping early based on specific criteria. Implementing callbacks requires coding knowledge.",
        "example": "See the documentation for examples of creating and using custom callbacks.",
        "proTip": "Use callbacks to add features like real-time logging, checkpointing, or early stopping. They’re powerful but require some programming expertise."
    },
    "learning_rate_start": {
        "title": "Initial Learning Rate",
        "description": "Controls how quickly the agent learns at the start of training. A balanced rate prevents overly aggressive or sluggish updates.",
        "example": "Example: Start with 0.0003 for a steady learning pace.",
        "proTip": "Begin with a small, stable value like 0.0003. Avoid values that are too high to ensure the agent doesn’t overshoot optimal strategies."
    },
    "learning_rate_end": {
        "title": "Final Learning Rate",
        "description": "Determines how slowly the agent learns as training progresses, allowing for careful fine-tuning in later stages.",
        "example": "Example: Set to 0.00005 for precise adjustments toward the end of training.",
        "proTip": "Set a smaller value like 0.00005 for controlled fine-tuning. Ensure it’s lower than the starting rate to stabilize late-stage learning."
    },
    "clip_range_vf_start": {
        "title": "Initial Value Function Clipping Range",
        "description": "Limits how much the value function can change early in training, ensuring stable learning from environment evaluations.",
        "example": "Example: Use 0.2 to allow reasonable but controlled updates.",
        "proTip": "Lower the range if value function updates cause instability. Use 'None' if unrestricted updates are preferred."
    },
    "seed": {
        "title": "Random Seed",
        "description": "Sets a seed to ensure reproducibility in training runs. With the same seed, the environment and results remain consistent.",
        "example": "Example: Use 42 for a reproducible setup to compare training outcomes.",
        "proTip": "Set a specific seed for debugging or benchmarking. Leave it random for unique runs each time."
    },
    "clip_range_vf_end": {
        "title": "Final Value Function Clipping Range",
        "description": "As training nears completion, this restricts value function updates to small, precise changes.",
        "example": "Example: Set to 0.05 for fine-grained tuning during final stages.",
        "proTip": "Use smaller values like 0.05 for stable late-stage training. Ensure this value is smaller than the initial range."
    },
    "pi_net": {
        "title": "Policy Network Architecture",
        "description": "Defines the architecture of the policy network, which guides the agent’s decisions. Each number represents the size of a layer.",
        "example": "Example: Use [256, 256] for two layers with 256 neurons each.",
        "proTip": "Larger architectures handle more complex tasks but require more computational resources. Start with [128, 128] and expand as needed."
    },
    "vf_net": {
        "title": "Value Function Network Architecture",
        "description": "Specifies the architecture of the value function network, which evaluates the overall potential of different states in the environment.",
        "example": "Example: Use [256, 256] for a balanced network with two layers of 256 neurons.",
        "proTip": "Align the value function network size with the policy network for balanced training. Larger networks suit more complex environments."
    },
    "training_config": {
        "title": "Training Configuration",
        "description": "Defines the key parameters for training, such as the number of environments, total timesteps, and stages the agent will explore.",
        "example": "Example: { 'num_envs': 4, 'total_timesteps': 1e7, 'stages': ['1-1', '1-2'] } for parallel training across multiple environments.",
        "proTip": "Increase 'num_envs' to speed up training if you have sufficient computational power. Tailor stage selection to specific training goals."
    },
    "hyperparameters": {
        "title": "Training Hyperparameters",
        "description": "Fine-tuned settings that control the agent’s learning process, such as how quickly it adapts, how many steps it takes before updating, and how much it values future rewards.",
        "example": "Example: { 'learning_rate': 3e-4, 'batch_size': 64, 'gamma': 0.99, 'clip_range': 0.2 } for balanced and stable training.",
        "proTip": "Start with default values and adjust incrementally if training is unstable or progress stalls. Small tweaks can lead to significant improvements."
    },
    "rollout_buffer_class": {
        "title": "Rollout Buffer Type",
        "description": "Specifies the type of buffer used to store experiences for training. The default buffer is optimized for most scenarios and requires no adjustments.",
        "example": "Example: Advanced users might use 'DictRolloutBuffer' for environments with custom data structures.",
        "proTip": "Unless implementing custom environments, stick to the default buffer for reliable performance."
    },
    "rollout_buffer_kwargs": {
        "title": "Rollout Buffer Settings",
        "description": "Additional configuration for the rollout buffer, affecting how experiences are stored and processed. Default settings work for most use cases.",
        "example": "Example: Set { 'gamma': 0.98, 'gae_lambda': 0.95 } to fine-tune reward processing in advanced setups.",
        "proTip": "This is an advanced setting for fine-tuning experience storage. Leave it at default unless working on highly customized tasks."
    },
    "target_kl": {
        "title": "Target KL Divergence",
        "description": "Limits the extent of policy changes during training to ensure stability and consistent learning.",
        "example": "Example: Use 0.01 to allow careful, incremental updates to the policy.",
        "proTip": "Set small values like 0.01 for better control over policy adjustments. This is optional for most tasks."
    },
    "shader-options": {
        "title": "Shader Options",
        "description": "Aesthetic customizations for the training environment visuals. These effects, like scanlines and CRT distortion, enhance the visual experience but don’t impact training performance.",
        "example": "Example: Enable 'Scanlines' for a retro arcade look or 'Radial Distortion' for a CRT-style display. Available options include:\n- Enable All: Toggle all shaders.\n- Radial Distortion: Adds a curved screen effect.\n- Scanlines: Creates horizontal lines for a retro monitor look.\n- Dot Mask: Mimics the RGB dot pattern of classic displays.\n- Rolling Lines: Simulates analog signal interference.\n- Gamma Correction: Adjusts brightness and contrast for vintage visuals.",
        "proTip": "Experiment with shader combinations to customize the display for a nostalgic gaming experience. These changes are purely aesthetic."
    },
    "game-render": {
        "title": "Game Renderer",
        "description": "Displays a real-time view of the agent’s experience during training, offering insights into its interactions and behavior in the environment.",
        "example": "Example: Watch the agent navigate obstacles, collect rewards, and react to challenges in real time. This provides a visual representation of the agent’s progress.",
        "proTip": "Use the renderer to monitor training visually. Aesthetic enhancements like shaders can be applied here without affecting the agent’s performance."
    },
    "no_attack_buttons_combinations": {
        "title": "Restrict Attack Buttons",
        "description": "Disables attack button combinations, limiting actions to single-button presses. Useful for simplifying action spaces.",
        "example": "Example: Set `no_attack_buttons_combinations = True` to ensure only single-button attacks are used.",
        "proTip": "Use this wrapper to reduce complexity in environments with many button combinations."
    },
    "no_op_max": {
        "title": "No-Action Steps After Reset",
        "description": "Performs a random number of 'no-op' actions (up to the maximum specified) after an episode reset to introduce randomness.",
        "example": "Example: Set `no_op_max = 6` to allow up to 6 no-op steps after resetting the environment.",
        "proTip": "Useful for environments where random starting conditions improve exploration."
    },
    "frame_shape": {
        "title": "Frame Reshaping (Deprecated)",
        "description": "Resizes frames or converts them to grayscale. Consider using the `frame_shape` environment setting for improved performance.",
        "example": "Example: Set `frame_shape = (128, 128, 1)` for grayscale frames resized to 128x128.",
        "proTip": "Use for optimizing input dimensions. Replace with engine-side settings where possible for better performance."
    },
    "stack_frames": {
        "title": "Stack Frames",
        "description": "Combines the latest N frames into a single observation along the depth dimension, providing temporal context to the agent.",
        "example": "Example: Set `stack_frames = 4` to stack the last 4 frames.",
        "proTip": "Use in dynamic environments where context from previous frames is critical for decision-making."
    },
    "dilation": {
        "title": "Frame Stacking Dilation",
        "description": "Determines the interval at which frames are stacked, skipping N-1 frames between each stack.",
        "example": "Example: Set `dilation = 2` to stack every second frame.",
        "proTip": "Combine with `stack_frames` for efficient frame stacking in high-FPS environments."
    },
    "add_last_action": {
        "title": "Add Last Action to Observation",
        "description": "Includes the last performed action in the observation space, helping the agent learn dependencies between actions and outcomes.",
        "example": "Example: Set `add_last_action = True` to add the last action to observations.",
        "proTip": "Activating this wrapper is required for the `Stack Actions` wrapper."
    },
    "stack_actions": {
        "title": "Stack Actions",
        "description": "Stacks the last N actions into the observation space, creating a MultiDiscrete space with N elements.",
        "example": "Example: Set `stack_actions = 12` to stack the last 12 actions.",
        "proTip": "Requires `add_last_action = True` to work. Use for agents that benefit from a history of past actions."
    },
    "scale": {
        "title": "Scale Observation",
        "description": "Normalizes observations to be within [0, 1]. Handles various data types like Box, Discrete, and MultiDiscrete spaces.",
        "example": "Example: Set `scale = True` to activate observation scaling.",
        "proTip": "Use with `exclude_image_scaling` and `process_discrete_binary` for fine-grained control over scaling."
    },
    "exclude_image_scaling": {
        "title": "Exclude Frame Scaling",
        "description": "Prevents scaling from being applied to image-based observations like game frames.",
        "example": "Example: Set `exclude_image_scaling = True` to skip scaling frames.",
        "proTip": "Useful in environments where frame data should remain in its original form."
    },
    "process_discrete_binary": {
        "title": "Process Binary Observations",
        "description": "Transforms binary Discrete observations into MultiBinary spaces using one-hot encoding.",
        "example": "Example: Set `process_discrete_binary = True` to one-hot encode binary Discrete observations.",
        "proTip": "Use when binary spaces need encoding for compatibility with the agent."
    },
    "role_relative": {
        "title": "Role-Relative Observation",
        "description": "Renames observation keys to reflect the agent’s role, standardizing observations across roles.",
        "example": "Example: Set `role_relative = True` to rename keys like P1 and P2 to 'own' and 'opp'.",
        "proTip": "Simplifies training in multi-agent scenarios by aligning observation structures."
    },
    "flatten": {
        "title": "Flatten Observation",
        "description": "Converts nested dictionaries in observations into flat structures with concatenated keys.",
        "example": "Example: Set `flatten = True` to flatten keys like `obs['P1']['key']` into `obs['P1_key']`.",
        "proTip": "Combine with `filter_keys` to focus on specific observation elements."
    },
    "filter_keys": {
        "title": "Filter Observation Keys",
        "description": "Defines a subset of observation keys to retain, removing unnecessary elements.",
        "example": "Example: Set `filter_keys = ['stage', 'P1_health']` to retain only specific keys.",
        "proTip": "Requires `flatten = True` to work. Use to streamline observation data."
    },
    "repeat_action": {
        "title": "Repeat Actions",
        "description": "Repeats the last action for N steps, simulating extended action durations.",
        "example": "Example: Set `repeat_action = 4` to repeat actions for 4 steps.",
        "proTip": "Requires `step_ratio = 1`. Useful for environments where repeated actions are common."
    },
    "normalize_reward": {
        "title": "Normalize Rewards",
        "description": "Scales rewards based on a normalization factor and the health bar range of the game.",
        "example": "Example: Set `normalize_reward = True` and `normalization_factor = 0.5` for scaled rewards.",
        "proTip": "Improves reward consistency across different games or scenarios."
    },
    "clip_reward": {
        "title": "Clip Rewards",
        "description": "Applies a sign function to rewards, reducing their magnitude to +1, 0, or -1.",
        "example": "Example: Set `clip_reward = True` to enable reward clipping.",
        "proTip": "Useful for simplifying reward structures in environments with high variability."
    },
    "wrapper-settings": {
        "title": "Diambra Wrappers",
        "description": "DIAMBRA Arena includes a variety of built-in wrappers designed to enhance training and gameplay. These wrappers cater to diverse use cases and provide templates for creating custom solutions.",
        "example": "Example: Enable `no_op_max` to introduce random delays after resets, or use `stack_frames` to add temporal context to observations.",
        "proTip": "This program is optimized so you can't make a wrong choice. Feel free to experiment and find what works best for your setup!"
    }
}
